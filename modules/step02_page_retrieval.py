import os
import json
import hashlib
import argparse
import pandas as pd
from retry import retry
from tqdm import tqdm
import random
import ast
from utils.openai_helper import initialize_client
from joblib import Parallel, delayed  # For parallelization
from utils.pipeline_utils import generate_hash
import re
import openai

def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Document page retrieval with LLMs based on page summaries")
    
    # Data paths
    parser.add_argument("--input_file", type=str, default="data/MMLongBench/samples.json",
                        help="Input JSON file with questions and document references")
    parser.add_argument("--summaries_dir", type=str, default="outputs/qwen25_32b/summaries",
                        help="Directory containing the page summaries generated by all_page_summarizer.py")
    parser.add_argument("--output_dir", type=str, default="outputs/qwen25_32b/retrievals",
                        help="Output directory to save result files")
    
    # New argument for query updates from step03_target_page_qa.py
    parser.add_argument("--qa_results_file", type=str, default=None,
                        help="JSONL file containing QA results with query updates from step03_target_page_qa.py")
    parser.add_argument("--updated_output_file", type=str, default=None,
                        help="Output file for updated retrievals (defaults to 'updated_retrievals.json' in output_dir)")
    
    # Model configuration
    parser.add_argument("--model", type=str, default="Qwen/Qwen3-30B-A3B",
                        help="Model name to use")
    parser.add_argument("--api_key_file", type=str, default="./deepinfrakey",
                        help="File containing the API key")
    parser.add_argument("--base_url", type=str, 
                        default="http://cn-w-1.hpc.engr.oregonstate.edu:8000/v1",
                        help="Base URL for API")
    parser.add_argument("--cache_seed", type=int, default=123,
                        help="Seed for OpenAI cache")
    
    # Model parameters
    parser.add_argument("--max_tokens", type=int, default=32768,
                        help="Maximum tokens for LLM response")
    parser.add_argument("--prompt_file", type=str, default="prompts/page_retrieval_prompt.txt",
                        help="Path to the page retrieval prompt file")
    
    # Subsampling parameter
    parser.add_argument("--subsample", type=int, default=None,
                        help="Number of samples to randomly select from valid questions. If None, use all samples.")
    
    # Parallelization parameter
    parser.add_argument("--n_jobs", type=int, default=32,
                        help="Number of parallel jobs for query processing. Default is 32.")
    
    # Verbosity parameter
    parser.add_argument("--verbose", action="store_true", default=False,
                        help="Enable verbose output")
    
    return parser.parse_args()

def query_llm_for_page_retrieval(question, page_summaries, model_name, client, max_tokens, prompt_file):
    """
    Sends a query and page summaries to the LLM to retrieve relevant pages.

    Args:
        question (str): The user query.
        page_summaries (dict): Dictionary of page summaries keyed by page number.
        model_name (str): The model to use.
        client: The API client wrapper.
        max_tokens (int): Maximum tokens for response.
        prompt_file (str): Path to the prompt template file.

    Returns:
        openai.ChatCompletionMessage: The response from the LLM.
    """
    print("Using the qwen llm func")
    with open(prompt_file, 'r') as f:
        prompt = f.read()

    # Format page summaries into a structured string
    formatted_summaries = ""
    for page_num, page_data in sorted(page_summaries.items(), key=lambda x: int(x[0])):
        if "summary" in page_data and page_data["summary"]:
            formatted_summaries += f"Page {page_num}: {page_data['summary']}\n\n"

    # Format the prompt with the page summaries and user question
    prompt = prompt.format(PAGE_SUMMARIES=formatted_summaries, USER_QUERY=question)

    estimated_input_tokens = len(prompt) // 4
    model_limit = 40960
    reserved = 500  # safety margin
    adjusted_max_tokens = max(min(max_tokens, model_limit - estimated_input_tokens - reserved), 256)

    try:
        response = client.create(
            model=model_name,
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.6,
            top_p=0.95,
            extra_body={"top_k": 20, "min_p": 0}, # , "chat_template_kwargs": {"enable_thinking": False}
            max_tokens=adjusted_max_tokens,
            # reasoning_effort='none',
        )
    except openai.APIError or TimeoutError as e:
        print(f"API error: {e}")
        return None

    if response.choices[0].finish_reason == "length":
        print("Response length exceeded the limit.")
        return None

    message = response.choices[0].message
    if message.content is None:
        print("Received empty message content from LLM.")
        return None


    return response.choices[0].message


def extract_relevant_pages(response):
    """
    Extract the selected pages from the LLM response (within <selected_pages> tags).
    
    Args:
        response (str): The full response from the LLM.
        
    Returns:
        list: The extracted relevant page numbers as a list, or empty list if extraction fails.
    """
    print("Using the extract_relevant_page func")
    try:
        # Extract content between <selected_pages> tags
        start_idx = response.find('<selected_pages>')
        end_idx = response.find('</selected_pages>')
        
        if start_idx != -1:
            pages_str = response[start_idx + len('<selected_pages>'):end_idx].strip()
            
            # Extract numbers using regex pattern
            numbers = re.findall(r'\d+', pages_str)
            page_indices = [int(num) for num in numbers]
            
            return page_indices
        else:
            return []
    except Exception as e:
        print(f"Error extracting relevant pages: {e}")
        return []

def extract_document_summary(response):
    """
    Extract the document summary from the LLM response (within <document_summary> tags).
    
    Args:
        response (str): The full response from the LLM.
        
    Returns:
        str: The extracted document summary, or empty string if extraction fails.
    """
    print("Using the extract_document_summary func")
    try:
        # Extract content between <document_summary> tags
        start_idx = response.find('<document_summary>')
        end_idx = response.find('</document_summary>')
        
        if start_idx != -1 and end_idx != -1:
            summary = response[start_idx + len('<document_summary>'):end_idx].strip()
            return summary
        else:
            return ""
    except Exception as e:
        print(f"Error extracting document summary: {e}")
        return ""

def process_document_query(doc_id, question, summary_file_path, args, client=None):
    """
    Process a document-query pair to retrieve relevant pages.
    
    Args:
        doc_id (str): Document ID
        question (str): Query for page retrieval
        summary_file_path (str): Path to the summary JSON file
        args: Command line arguments
        client: OpenAI client wrapper (if None, will create a new client for thread safety)
        
    Returns:
        dict: Results with the relevant pages and other metadata
    """
    try:
        # Create a new client for each worker to ensure thread safety
        if client is None:
            local_client = initialize_client(args)
        else:
            local_client = client
            
        if local_client is None:
            return {
                'doc_id': doc_id,
                'question': question,
                'error': 'Failed to initialize client',
                'relevant_pages': [],
                'document_summary': ""
            }
            
        # Load the summary file
        with open(summary_file_path, 'r', encoding='utf-8') as f:
            summary_data = json.load(f)
        
        # Get the page summaries
        page_summaries = summary_data.get('pages', {})
        
        if not page_summaries:
            return {
                'doc_id': doc_id,
                'question': question,
                'error': 'No page summaries found',
                'relevant_pages': [],
                'document_summary': ""
            }
        
        # Generate a retrieval result for this query
        message = query_llm_for_page_retrieval(
            question, 
            page_summaries, 
            args.model, 
            local_client, 
            args.max_tokens, 
            args.prompt_file
        )
        if message is None:
            return {
                'doc_id': doc_id,
                'question': question,
                'error': 'Failed to get retrieval response from LLM',
                'relevant_pages': [],
                'document_summary': "",
                'llm_response': "",
                'reasoning': ""
            }

        retrieval_response, reasoning = message.content, message.model_extra["reasoning_content"]

        # Extract the relevant pages
        relevant_pages = extract_relevant_pages(retrieval_response)
        
        # Extract the document summary
        document_summary = extract_document_summary(retrieval_response)

        if len(relevant_pages) == 0:
            print(f"No relevant pages found for {doc_id}, question '{question}'")
            print(f"Reasoning Details: {reasoning}")
            
            # Return with error information when no pages are found
            return {
                'doc_id': doc_id,
                'question': question,
                'error': 'No relevant pages found',
                'relevant_pages': [],
                'document_summary': document_summary,
                'llm_response': retrieval_response,
                'reasoning': reasoning
            }
        
        if args.verbose:
            print(f"Retrieved pages for {doc_id}, question '{question}': {relevant_pages}")
            if document_summary:
                print(f"Document summary: {document_summary}")
        
        # Return without llm_response for successful cases
        return {
            'doc_id': doc_id,
            'question': question,
            'relevant_pages': relevant_pages,
            'document_summary': document_summary
        }
    
    except Exception as e:
        if args.verbose:
            print(f"Error processing document {doc_id}, question '{question}': {e}")
        return {
            'doc_id': doc_id,
            'question': question,
            'error': str(e),
            'relevant_pages': [],
            'document_summary': "",
            'llm_response': retrieval_response if 'retrieval_response' in locals() else "",
            'reasoning': reasoning if 'reasoning' in locals() else ""
        }

def retrieve_relevant_pages(dataset_df, args, client):
    """
    Processes document-query pairs to retrieve relevant pages based on summaries.
    Creates a consolidated JSON file with retrieval results for all document-question pairs.
    Uses parallel processing for improved performance.
    
    Args:
        dataset_df (pd.DataFrame): DataFrame containing the dataset
        args: Command line arguments
        client: OpenAI client wrapper
    """
    # Create output directory if it doesn't exist
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Define consolidated file path
    consolidated_file = os.path.join(args.output_dir, "all_retrievals.json")
    
    # Initialize results list
    results = []
    
    # Prepare data for parallel processing
    query_data = []
    for idx, row in dataset_df.iterrows():
        doc_id = row['doc_id']
        question = row['question']
        
        # Define path for input summary file - now using document ID directly
        summary_file = os.path.join(args.summaries_dir, f"{doc_id}.json")
        
        # Check if summary file exists
        if not os.path.exists(summary_file):
            print(f"Error: Summary file not found for document {doc_id} at {summary_file}")
            error_result = {
                'doc_id': doc_id,
                'question': question,
                'error': 'Summary file not found',
                'relevant_pages': [],
                'document_summary': ""
            }
            results.append(error_result)
        else:
            # Add to query data for processing
            query_data.append((doc_id, question, summary_file))
    
    # Process the queries in parallel
    print(f"Processing {len(query_data)} document-question pairs in parallel with {args.n_jobs} jobs...")
    
    # Use Parallel to process the queries
    # Don't pass the client to ensure each worker creates its own client instance for thread safety
    parallel_results = Parallel(n_jobs=args.n_jobs, backend="threading")(
        delayed(process_document_query)(
            doc_id, question, summary_file, args, None
        ) for doc_id, question, summary_file in tqdm(query_data, desc="Processing Queries")
    )
    
    # Combine the results
    results.extend(parallel_results)
    
    # Write all results to the consolidated file
    with open(consolidated_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nSaved all retrieval results to {consolidated_file}")
    print("\n--- Processing Complete ---")
    
    return results

def evaluate_retrievals(dataset_df, results):
    """
    Evaluate the retrieval performance against ground truth evidence pages.
    
    Args:
        dataset_df (pd.DataFrame): DataFrame containing the dataset with ground truth
        results (list): List of retrieval results
        
    Returns:
        dict: Evaluation metrics
    """
    total = 0
    correct = 0
    partial = 0
    
    for result in results:
        doc_id = result['doc_id']
        question = result['original_question'] if 'original_question' in result else result['question']
        
        # Find the corresponding row in the dataset
        row = dataset_df[(dataset_df['doc_id'] == doc_id) & (dataset_df['question'] == question)]
        
        if row.empty or 'error' in result:
            continue
        
        total += 1
        
        # Get the predicted relevant pages
        predicted_pages = set(result.get('relevant_pages', []))
        
        # Get the ground truth evidence pages
        gt_pages_str = row['evidence_pages'].iloc[0]
        
        # Parse ground truth pages
        if gt_pages_str and gt_pages_str != "[]":
            # Handle various formats of evidence_pages
            try:
                # Try to parse as a list string
                gt_pages = ast.literal_eval(gt_pages_str)
                gt_pages = [int(p) for p in gt_pages]
            except:
                # Try to extract numbers from the string
                gt_pages = []
                for part in gt_pages_str.strip("[]").split(","):
                    try:
                        gt_pages.append(int(part.strip()))
                    except:
                        pass
        else:
            gt_pages = []
        
        gt_pages = set(gt_pages)
        
        # Compute metrics
        if predicted_pages:
            if predicted_pages == gt_pages:
                correct += 1
            elif predicted_pages >= gt_pages:
                partial += 1
    
    # Compute overall metrics
    if total > 0:
        accuracy = correct / total
        partial_recall = (correct + partial) / total
    else:
        accuracy = 0
        partial_recall = 0
    
    return {
        'total_evaluated': total,
        'exact_match': correct,
        'partial_match': partial,
        'accuracy': accuracy,
        'partial_recall': partial_recall
    }

def load_qa_results_with_query_updates(qa_results_file):
    """
    Load QA results from a JSONL file and extract items with query_update response type.
    
    Args:
        qa_results_file (str): Path to the JSONL file with QA results
        
    Returns:
        list: List of dictionaries containing doc_id, original question, and updated query
    """
    # Read JSONL file using pandas
    df = pd.read_json(qa_results_file, lines=True)
    
    # Filter for query updates and create list of dictionaries
    query_updates = df[df['response_type'] == 'query_update'].apply(
        lambda row: {
            'doc_id': row['doc_id'],
            'original_question': row['question'],
            'updated_query': row['processed_response']
        }, axis=1
    ).tolist()
    
    print(f"Loaded {len(query_updates)} query updates from {qa_results_file}")
    return query_updates

def process_updated_queries(query_updates, args, client):
    """
    Process document queries with updated queries from QA results.
    
    Args:
        query_updates (list): List of dictionaries with doc_id, original_question, and updated_query
        args: Command line arguments
        client: OpenAI client wrapper
        
    Returns:
        list: List of retrieval results for updated queries
    """
    # Create output directory if it doesn't exist
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Prepare data for parallel processing
    query_data = []
    for item in query_updates:
        doc_id = item['doc_id']
        updated_query = item['updated_query']
        
        # Define path for input summary file
        summary_file = os.path.join(args.summaries_dir, f"{doc_id}.json")
        
        # Check if summary file exists
        if not os.path.exists(summary_file):
            print(f"Error: Summary file not found for document {doc_id} at {summary_file}")
        else:
            # Add to query data for processing
            query_data.append((doc_id, updated_query, summary_file))
    
    # Process the queries in parallel
    print(f"Processing {len(query_data)} updated queries in parallel with {args.n_jobs} jobs...")
    
    # Use Parallel to process the queries
    parallel_results = Parallel(n_jobs=args.n_jobs, backend="threading")(
        delayed(process_document_query)(
            doc_id, query, summary_file, args, None
        ) for doc_id, query, summary_file in tqdm(query_data, desc="Processing Updated Queries")
    )
    
    # Add original question to each result for reference
    for result, item in zip(parallel_results, query_updates):
        result['original_question'] = item['original_question']
    
    return parallel_results

def main():
    """Main function to run the document page retrieval process."""
    # Parse command line arguments
    args = parse_arguments()
    
    # Initialize the client
    client = initialize_client(args)
    if client is None:
        print("Exiting script as client could not be initialized.")
        return
    
    # Check if we're processing query updates from QA results
    if args.qa_results_file:
        print(f"Processing query updates from QA results file: {args.qa_results_file}")
        
        # Load query updates from QA results
        query_updates = load_qa_results_with_query_updates(args.qa_results_file)
        
        if not query_updates:
            print("No query updates found. Exiting.")
            return
        
        # Process updated queries
        updated_results = process_updated_queries(query_updates, args, client)
        
        # Set the output file for updated retrievals
        if args.updated_output_file:
            updated_output_file = args.updated_output_file
        else:
            updated_output_file = os.path.join(args.output_dir, "updated_retrievals.json")
        
        # Write updated results to file
        with open(updated_output_file, 'w', encoding='utf-8') as f:
            json.dump(updated_results, f, ensure_ascii=False, indent=2)

        dataset_df = pd.read_json(args.input_file)
        print(args.input_file)
        # Evaluate the retrieval performance
        eval_metrics = evaluate_retrievals(dataset_df, updated_results)

        print("\n--- Evaluation Results ---")
        print(f"Total evaluated: {eval_metrics['total_evaluated']}")
        print(f"Exact matches: {eval_metrics['exact_match']} ({eval_metrics['accuracy']:.2%})")
        print(f"Partial matches: {eval_metrics['partial_match']} ({eval_metrics['partial_recall']:.2%})")

        print(f"Saved {len(updated_results)} updated retrieval results to {updated_output_file}")
        return

    # Regular processing flow for initial queries
    # Load the dataset
    dataset_df = pd.read_json(args.input_file)

    valid_samples = list(range(len(dataset_df)))
    
    # Apply subsampling if requested
    if args.subsample is not None and args.subsample < len(valid_samples):
        random.seed(42)  # For reproducibility
        selected_indices = random.sample(valid_samples, args.subsample)
        print(f"Randomly selected {args.subsample} samples for processing")
    else:
        selected_indices = valid_samples
        if args.subsample is not None:
            print(f"Requested {args.subsample} samples but only {len(valid_samples)} valid samples available")
    
    # Filter the dataset to include only the selected samples
    filtered_dataset_df = dataset_df.iloc[selected_indices].reset_index(drop=True)
    
    # Process the document-query pairs and get results
    all_results = retrieve_relevant_pages(filtered_dataset_df, args, client)
    
    # Evaluate the retrieval performance
    eval_metrics = evaluate_retrievals(dataset_df, all_results)
    
    print("\n--- Evaluation Results ---")
    print(f"Total evaluated: {eval_metrics['total_evaluated']}")
    print(f"Exact matches: {eval_metrics['exact_match']} ({eval_metrics['accuracy']:.2%})")
    print(f"Partial matches: {eval_metrics['partial_match']} ({eval_metrics['partial_recall']:.2%})")
    
    print(f"All document page retrievals have been saved to '{args.output_dir}/all_retrievals.json'")

if __name__ == "__main__":
    main() 